#install.packages("tm") 
#install.packages("libxml2") 
#install.packages("corpustools") 
#install.packages("mongolite") 
#install.packages("dplyr") 
#install.packages("fastrtext") 

library(mongolite)
library(dplyr) 
library(tm)
library(corpustools)
library(fastrtext)

### Remove variables if they exist
if (exists("m")) {rm(m)}
if (exists("getdata")) {rm(getdata)}
if (exists("listid")) {rm(listid)}
if (exists("listtext")) {rm(listtext)}
if (exists("textframe")) {rm(textframe)}
if (exists("corpus")) {rm(corpus)}
if (exists("corpusmatrix")) {rm(corpusmatrix)}
if (exists("model")) {rm(model)}
if (exists("predictions")) {rm(predictions)}
if (exists("test_labels_without_prefix")) {rm(test_labels_without_prefix)}
if (exists("test_sentences")) {rm(test_sentences)}
if (exists("textframe2")) {rm(textframe2)}
if (exists("train_sentences")) {rm(train_sentences)}
if (exists("corpusdataframe")) {rm(corpusdataframe)}
if (exists("listtest")) {rm(listtest)}

### Verbindung zu Kategorien in MongoDB (wird aktuell nicht gebraucht)
# m <- mongo("categories", url = "mongodb://192.168.2.135:27017/mails")  

### Verbindung zu den Mails in MongoDB
# m <- mongo("interactions", url = "mongodb://192.168.2.135:27017/mails")
m <- mongo("interactions", url = "mongodb://localhost:27017/mails")
getdata <- m

### DurchzÃ¤hlen wie viele DatensÃ¤tze vorhanden sind
getdata$count('{}')

### Texte und Kategorien aus MongoDB abfragen und bereinigen (da aktuell nicht sauber gespeichert)
# Kategorien
listid <- getdata$find(query = '{}', fields = '{"categories.id" : true, "_id": false}')
listid <- data.frame(lapply(listid, function(x) {gsub("list\\(id = \"", "", x)}))
listid <- data.frame(lapply(listid, function(x) {gsub("\"\\)", "", x)}))
listid <- data.frame(lapply(listid, function(x) {gsub("list\\(id = c\\(\"", "", x)}))
listid <- data.frame(lapply(listid, function(x) {gsub("\"", "", x)}))
listid <- data.frame(lapply(listid, function(x) {gsub("\\)", "", x)}))

### Relevante Textbausteine
listtext <- getdata$find(query = '{}',fields = '{"categories.text" : true, "_id": false}')
listtext <- data.frame(lapply(listtext, function(x) {gsub("list\\(text = \"", "", x)}))
listtext <- data.frame(lapply(listtext, function(x) {gsub("\"\\)", "", x)}))

### Dataframes zusammenfÃ¼hren und anpassen
textframe <- data.frame(listid, listtext)
colnames(textframe) <- c("id", "text")
# print(textframe)

### wir Laden die 2te Spalte in den Dataframe und verwenden tm zur Breinigung
corpus <- VCorpus(VectorSource(textframe[,2]))

# inspect(corpus)

### whitespace entfernen
corpus <- tm_map(corpus, stripWhitespace)
### convert to lowercases
corpus <- tm_map(corpus, content_transformer(tolower))
### remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("german"))
### stemming
tm_map(corpus, stemDocument)

corpusdataframe <- data.frame(text_new = sapply(corpus, as.character), stringsAsFactors = FALSE)

# inspect(corpus)
# print(textframe)

### corpusmatrix erst einmal weglassen, wird nicht benÃ¶tigt fÃ¼r fastrtext
### Term document Matrices
# corpusmatrix <- DocumentTermMatrix(corpus)
# inspect(corpusmatrix)

### HÃ¤ufigkeitsmatrix
# findFreqTerms(corpusmatrix, 10)

### Korrelationen zwischen einzelnen WÃ¶rtern
# findAssocs(corpusmatrix, "fehler", 0.5)

### wir entfernen sehr seltene WÃ¶rter, die unwichtig bzgl. der Korrelation sind
# removeSparseTerms(corpusmatrix, 0.4)
# inspect(corpusmatrix)

### wir schreiben in eine neue Spalte die editierten Texte
textframe[,3] <- corpusdataframe

##### FASTRTEXT #####

### fuer fastrtext benÃ¶tigen wir die Daten in einem bestimmten Format, dafÃ¼r laden wir die benÃ¶tigten Spalten in 
### ein neues Dataframe
textframe2 <- subset(textframe, select=c("id", "text_new"))
names(textframe2) <- c("class.text", "text")

### Wir mixen den Datensatz durch vor dem Splitten (bei 70/30 Aufteilung pro Kategorie nicht mehr benötigt)
# textframe2 <- textframe2[sample(nrow(textframe2)),]

### Alle Zeilen mit mehreren IDs lÃ¶schen, nicht sauber aber die mÃ¼ssen erst einmal weg
textframe2 <- textframe2[!grepl(",", textframe2$class.text),]

### hier zÃ¤hle ich einmalig die Anzahl der Texte pro Kategorie um sie danach zu lÃ¶schen
# TODO: das muss man automatisieren
# textframe3 <- aggregate(cbind(count = text) ~ class.text, data = textframe2, FUN = function(x){NROW(x)})
# delete categories 59, 70, 72, 85, 86, 87, 92, 97
textframe2 <- textframe2[!grepl("59", textframe2$class.text),]
textframe2 <- textframe2[!grepl("70", textframe2$class.text),]
textframe2 <- textframe2[!grepl("72", textframe2$class.text),]
textframe2 <- textframe2[!grepl("85", textframe2$class.text),]
textframe2 <- textframe2[!grepl("86", textframe2$class.text),]
textframe2 <- textframe2[!grepl("87", textframe2$class.text),]
textframe2 <- textframe2[!grepl("92", textframe2$class.text),]
textframe2 <- textframe2[!grepl("97", textframe2$class.text),]

### START OF training and test dataset generation ###
textframe_ordered <- textframe2[order(textframe2$class.text, decreasing = FALSE), ]  
textframe_ordered$class.text <- as.numeric(as.character(textframe_ordered$class.text))
# str(textframe_ordered)
# View(textframe_ordered)
splits <- split(textframe_ordered, textframe_ordered$class.text)
# View(splits)

### Splitting whole dataset in 70/30 (not necessary as cateogires are splitted itself in 70/30)
#train_sentences <- head(textframe2, 450)
#test_sentences <- tail(textframe2, 145)

### function to obtain first 70 percent of data (with round off)
upperFunction <- function(textframes){
  if(nrow(textframes)>0){
    head(textframes, ceiling(nrow(textframes)*0.7))
  }
}

### function to obtain last 30 percent of data (with round off)
lowerFunction <- function(textframes){
  if(nrow(textframes)>0){
    tail(textframes, floor(nrow(textframes)*0.3))
  }
}

### generatíng train and test dataset
train_sentences <- lapply(splits, upperFunction)
test_sentences <- lapply(splits, lowerFunction)
# str(train_sentences)
# str(test_sentences)

### Combine data frames in list to one dataframe
train_set <- bind_rows(train_sentences)
test_set <- bind_rows(test_sentences)
# str(train_set)
# str(test_set)
# View(train_set)
# View(test_set)

### END OF training and test dataset generation ###

### TODO: das muss verbessert werden
# textframe2 %>% 
# add_count(class.text)

### Daten in richtiges Format fÃ¼r Model
tmp_file_model <- tempfile()
?tempfile()

### Label vor die Kategorien
train_labels <- paste0("__label__", train_set[,"class.text"])


### Text in Kleinbuchstaben
train_texts <- tolower(train_set[,"text"])
# View(train_texts)

### Kategorie mit Prefix + Text aus Kleinbuchstaben
train_to_write <- paste(train_labels, train_texts)
# View(train_to_write)
train_tmp_file_txt <- tempfile()

### Daten in tempfile schreiben
writeLines(text = train_to_write, con = train_tmp_file_txt)


### gleiches Vorgehen fÃ¼r Testdatensatz
test_labels <- paste0("__label__", test_set[,"class.text"])
test_labels_without_prefix <- test_set[,"class.text"]
test_texts <- tolower(test_set[,"text"])
test_to_write <- paste(test_labels, test_texts)


### Modell trainieren und in tempfile abspeichern; Parameter kÃ¶nnen angepasst werden 
execute(commands = c("supervised", "-input", train_tmp_file_txt, "-output", tmp_file_model, "-dim", 100, "-lr", 1, "-epoch", 150, "-wordNgrams", 2, "-verbose", 2))

### Modell laden
model <- load_model(tmp_file_model)

### Modell auf Testdatensatz anwenden
predictions <- predict(model, sentences = test_to_write)

### Vorhergesagte Kategorie und Wahrscheinlichkeit ausgeben
print(head(predictions,100))
summary(unlist(predictions))

### Prozentualer Anteil, in dem das Model richtig lag
mean(names(unlist(predictions)) == test_labels_without_prefix)

### because there is only one category by observation, hamming loss will be the same
get_hamming_loss(as.list(test_labels_without_prefix), predictions)

### you can get flat list of results when you are retrieving only one label per observation
print(head(predict(model, sentences = test_to_write, simplify = TRUE)))

### free memory
unlink(train_tmp_file_txt)
unlink(tmp_file_model)
gc()

